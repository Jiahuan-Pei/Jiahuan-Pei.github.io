---
title: "ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues"

collection: publications
pubsource: proceeding
permalink: /publication/2022-01-01-ReMeDi-Resources-for-Multi-domain-Multi-service-Medical-Dialogues
excerpt: 'In summary, we contribute:(1) A dataset contains 96,965 conversations between doctors and patients, including 1,557 conversations with fine-gained labels. It covers 843 types of diseases, 5,228 medical entities, and 3 specialties of medical services across 40 domains. To the best of our knowledge, the ReMeDi dataset is the only medical dialogue dataset that covers multiple domains and services, and has fine-grained medical labels. (2) Benchmark methods: (a) pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5) trained, validated, and tested on the ReMeDi dataset, and (b) a self-supervised contrastive learning (SCL) method to expand the ReMeDi dataset and enhance the training of the state-of-the-art pretrained models.'
date: 2022-01-01
venue: 'In Proceedings of International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2022, CCF A)'
paperurl: 'https://arxiv.org/pdf/2109.00430.pdf'
codeurl: 'https://github.com/yanguojun123/Medical-Dialogue'
dataurl: 'https://github.com/yanguojun123/Medical-Dialogue/tree/main/data'
citation: ' Guojun Yan,  <b>Jiahuan Pei*</b>,  Pengjie Ren,  Zhaochun Ren,  Maarten Rijke, &quot;ReMeDi: Resources for Multi-domain, Multi-service, Medical Dialogues.&quot; In Proceedings of International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2022, CCF A), 2022.'
---
In summary, we contribute:(1) A dataset contains 96,965 conversations between doctors and patients, including 1,557 conversations with fine-gained labels. It covers 843 types of diseases, 5,228 medical entities, and 3 specialties of medical services across 40 domains. To the best of our knowledge, the ReMeDi dataset is the only medical dialogue dataset that covers multiple domains and services, and has fine-grained medical labels. (2) Benchmark methods: (a) pretrained models (i.e., BERT-WWM, BERT-MED, GPT2, and MT5) trained, validated, and tested on the ReMeDi dataset, and (b) a self-supervised contrastive learning (SCL) method to expand the ReMeDi dataset and enhance the training of the state-of-the-art pretrained models.
